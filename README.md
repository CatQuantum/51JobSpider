



# 爬虫：

save_search_pc_responses.py

mitmproxy脚本，抓取需要的response。



Selenium.py

Selenium脚本，完成在特定网站（51job搜索界面）的翻页功能，每次翻页都会产生一个response。



抓取排名信息：

一个静态的HTML网页，但使用responses库时出现了SSL相关的问题。最终用subprocess调用curl解决了。



# 数据清洗脚本

final_progressing.ipynb

完成指定的数据清洗功能



json2excel_all.py

把json文件转成excel



其他：

暂未整理好，整理完毕再发。



# 废弃

Creeps：仅使用Selenium实现，效果不太好。

older version：用于存放上次实验中使用过的代码。

